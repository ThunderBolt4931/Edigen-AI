{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a639acc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Sequence, Mapping, Any, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_value_at_index(obj: Union[Sequence, Mapping], index: int) -> Any:\n",
    "    \"\"\"Returns the value at the given index of a sequence or mapping.\n",
    "\n",
    "    If the object is a sequence (like list or string), returns the value at the given index.\n",
    "    If the object is a mapping (like a dictionary), returns the value at the index-th key.\n",
    "\n",
    "    Some return a dictionary, in these cases, we look for the \"results\" key\n",
    "\n",
    "    Args:\n",
    "        obj (Union[Sequence, Mapping]): The object to retrieve the value from.\n",
    "        index (int): The index of the value to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        Any: The value at the given index.\n",
    "\n",
    "    Raises:\n",
    "        IndexError: If the index is out of bounds for the object and the object is not a mapping.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return obj[index]\n",
    "    except KeyError:\n",
    "        return obj[\"result\"][index]\n",
    "\n",
    "\n",
    "def find_path(name: str, path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Recursively looks at parent folders starting from the given path until it finds the given name.\n",
    "    Returns the path as a Path object if found, or None otherwise.\n",
    "    \"\"\"\n",
    "    # If no path is given, use the current working directory\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "\n",
    "    # Check if the current directory contains the name\n",
    "    if name in os.listdir(path):\n",
    "        path_name = os.path.join(path, name)\n",
    "        print(f\"{name} found: {path_name}\")\n",
    "        return path_name\n",
    "\n",
    "    # Get the parent directory\n",
    "    parent_directory = os.path.dirname(path)\n",
    "\n",
    "    # If the parent directory is the same as the current directory, we've reached the root and stop the search\n",
    "    if parent_directory == path:\n",
    "        return None\n",
    "\n",
    "    # Recursively call the function with the parent directory\n",
    "    return find_path(name, parent_directory)\n",
    "\n",
    "\n",
    "def add_comfyui_directory_to_sys_path() -> None:\n",
    "    \"\"\"\n",
    "    Add 'ComfyUI' to the sys.path\n",
    "    \"\"\"\n",
    "    comfyui_path = find_path(\"ComfyUI\")\n",
    "    if comfyui_path is not None and os.path.isdir(comfyui_path):\n",
    "        sys.path.append(comfyui_path)\n",
    "        print(f\"'{comfyui_path}' added to sys.path\")\n",
    "\n",
    "\n",
    "def add_extra_model_paths() -> None:\n",
    "    \"\"\"\n",
    "    Parse the optional extra_model_paths.yaml file and add the parsed paths to the sys.path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from main import load_extra_path_config\n",
    "    except ImportError:\n",
    "        print(\n",
    "            \"Could not import load_extra_path_config from main.py. Looking in utils.extra_config instead.\"\n",
    "        )\n",
    "        from utils.extra_config import load_extra_path_config\n",
    "\n",
    "    extra_model_paths = find_path(\"extra_model_paths.yaml\")\n",
    "\n",
    "    if extra_model_paths is not None:\n",
    "        load_extra_path_config(extra_model_paths)\n",
    "    else:\n",
    "        print(\"Could not find the extra_model_paths config file.\")\n",
    "\n",
    "\n",
    "add_comfyui_directory_to_sys_path()\n",
    "add_extra_model_paths()\n",
    "\n",
    "\n",
    "def import_custom_nodes() -> None:\n",
    "    \"\"\"Find all custom nodes in the custom_nodes folder and add those node objects to NODE_CLASS_MAPPINGS\n",
    "\n",
    "    This function sets up a new asyncio event loop, initializes the PromptServer,\n",
    "    creates a PromptQueue, and initializes the custom nodes.\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    import execution\n",
    "    from nodes import init_extra_nodes\n",
    "    import server\n",
    "\n",
    "    # Creating a new event loop and setting it as the default loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Creating an instance of PromptServer with the loop\n",
    "    server_instance = server.PromptServer(loop)\n",
    "    execution.PromptQueue(server_instance)\n",
    "\n",
    "    # Initializing custom nodes\n",
    "    init_extra_nodes()\n",
    "\n",
    "\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "\n",
    "\n",
    "def main():\n",
    "    import_custom_nodes()\n",
    "    with torch.inference_mode():\n",
    "        vaeloader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "        vaeloader_107 = vaeloader.load_vae(vae_name=\"ae.safetensors\")\n",
    "\n",
    "        unetloader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
    "        unetloader_112 = unetloader.load_unet(\n",
    "            unet_name=\"flux1-fill-dev-fp8.safetensors\", weight_dtype=\"default\"\n",
    "        )\n",
    "\n",
    "        ksamplerselect = NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "        ksamplerselect_120 = ksamplerselect.get_sampler(sampler_name=\"dpmpp_2m\")\n",
    "\n",
    "        clipvisionloader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "        clipvisionloader_151 = clipvisionloader.load_clip(\n",
    "            clip_name=\"sigclip_vision_patch14_384.safetensors\"\n",
    "        )\n",
    "\n",
    "        stylemodelloader = NODE_CLASS_MAPPINGS[\"StyleModelLoader\"]()\n",
    "        stylemodelloader_152 = stylemodelloader.load_style_model(\n",
    "            style_model_name=\"flux1-redux-dev.safetensors\"\n",
    "        )\n",
    "\n",
    "        loadimage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "        loadimage_187 = loadimage.load_image(image=\"pexels-pixabay-68201 (2).jpg\")\n",
    "\n",
    "        cr_upscale_image = NODE_CLASS_MAPPINGS[\"CR Upscale Image\"]()\n",
    "        cr_upscale_image_190 = cr_upscale_image.upscale(\n",
    "            upscale_model=\"4x_NMKD-Siax_200k.pth\",\n",
    "            resampling_method=\"lanczos\",\n",
    "            supersample=\"true\",\n",
    "            image=get_value_at_index(loadimage_187, 0),\n",
    "        )\n",
    "\n",
    "        clipvisionencode = NODE_CLASS_MAPPINGS[\"CLIPVisionEncode\"]()\n",
    "        clipvisionencode_153 = clipvisionencode.encode(\n",
    "            crop=\"none\",\n",
    "            clip_vision=get_value_at_index(clipvisionloader_151, 0),\n",
    "            image=get_value_at_index(cr_upscale_image_190, 0),\n",
    "        )\n",
    "\n",
    "        loadconditioningnode = NODE_CLASS_MAPPINGS[\"LoadConditioningNode\"]()\n",
    "        loadconditioningnode_327 = loadconditioningnode.load_conditioning(\n",
    "            filename=\"prompt_conditioning.safetensors\"\n",
    "        )\n",
    "\n",
    "        stylemodelapplysimple = NODE_CLASS_MAPPINGS[\"StyleModelApplySimple\"]()\n",
    "        stylemodelapplysimple_106 = stylemodelapplysimple.apply_stylemodel(\n",
    "            image_strength=\"high\",\n",
    "            conditioning=get_value_at_index(loadconditioningnode_327, 0),\n",
    "            style_model=get_value_at_index(stylemodelloader_152, 0),\n",
    "            clip_vision_output=get_value_at_index(clipvisionencode_153, 0),\n",
    "        )\n",
    "\n",
    "        conditioningconcat = NODE_CLASS_MAPPINGS[\"ConditioningConcat\"]()\n",
    "        conditioningconcat_121 = conditioningconcat.concat(\n",
    "            conditioning_to=get_value_at_index(loadconditioningnode_327, 0),\n",
    "            conditioning_from=get_value_at_index(stylemodelapplysimple_106, 0),\n",
    "        )\n",
    "\n",
    "        loadimage_200 = loadimage.load_image(\n",
    "            image=\"clipspace/clipspace-mask-518788.3000000119.png [input]\"\n",
    "        )\n",
    "\n",
    "        cr_upscale_image_336 = cr_upscale_image.upscale(\n",
    "            upscale_model=\"4x_NMKD-Siax_200k.pth\",\n",
    "            resampling_method=\"lanczos\",\n",
    "            supersample=\"true\",\n",
    "            image=get_value_at_index(loadimage_200, 0),\n",
    "        )\n",
    "\n",
    "        masktoimage = NODE_CLASS_MAPPINGS[\"MaskToImage\"]()\n",
    "        masktoimage_170 = masktoimage.mask_to_image(\n",
    "            mask=get_value_at_index(loadimage_200, 1)\n",
    "        )\n",
    "\n",
    "        cr_upscale_image_337 = cr_upscale_image.upscale(\n",
    "            upscale_model=\"4x_NMKD-Siax_200k.pth\",\n",
    "            resampling_method=\"lanczos\",\n",
    "            supersample=\"true\",\n",
    "            image=get_value_at_index(masktoimage_170, 0),\n",
    "        )\n",
    "\n",
    "        imagetomask = NODE_CLASS_MAPPINGS[\"ImageToMask\"]()\n",
    "        imagetomask_171 = imagetomask.image_to_mask(\n",
    "            channel=\"red\", image=get_value_at_index(cr_upscale_image_337, 0)\n",
    "        )\n",
    "\n",
    "        inpaintcropimproved = NODE_CLASS_MAPPINGS[\"InpaintCropImproved\"]()\n",
    "        inpaintcropimproved_183 = inpaintcropimproved.inpaint_crop(\n",
    "            downscale_algorithm=\"bilinear\",\n",
    "            upscale_algorithm=\"bicubic\",\n",
    "            preresize=False,\n",
    "            preresize_mode=\"ensure minimum resolution\",\n",
    "            preresize_min_width=1024,\n",
    "            preresize_min_height=1024,\n",
    "            preresize_max_width=16384,\n",
    "            preresize_max_height=16384,\n",
    "            mask_fill_holes=True,\n",
    "            mask_expand_pixels=0,\n",
    "            mask_invert=False,\n",
    "            mask_blend_pixels=32,\n",
    "            mask_hipass_filter=0.1,\n",
    "            extend_for_outpainting=False,\n",
    "            extend_up_factor=1,\n",
    "            extend_down_factor=1,\n",
    "            extend_left_factor=1,\n",
    "            extend_right_factor=1,\n",
    "            context_from_mask_extend_factor=1.2000000000000002,\n",
    "            output_resize_to_target_size=True,\n",
    "            output_target_width=1080,\n",
    "            output_target_height=1080,\n",
    "            output_padding=\"128\",\n",
    "            image=get_value_at_index(cr_upscale_image_336, 0),\n",
    "            mask=get_value_at_index(imagetomask_171, 0),\n",
    "        )\n",
    "\n",
    "        getimagesize = NODE_CLASS_MAPPINGS[\"GetImageSize+\"]()\n",
    "        getimagesize_179 = getimagesize.execute(\n",
    "            image=get_value_at_index(inpaintcropimproved_183, 1)\n",
    "        )\n",
    "\n",
    "        imageresize = NODE_CLASS_MAPPINGS[\"ImageResize+\"]()\n",
    "        imageresize_181 = imageresize.execute(\n",
    "            width=16384,\n",
    "            height=get_value_at_index(getimagesize_179, 1),\n",
    "            interpolation=\"lanczos\",\n",
    "            method=\"keep proportion\",\n",
    "            condition=\"always\",\n",
    "            multiple_of=0,\n",
    "            image=get_value_at_index(cr_upscale_image_190, 0),\n",
    "        )\n",
    "\n",
    "        imageconcanate = NODE_CLASS_MAPPINGS[\"ImageConcanate\"]()\n",
    "        imageconcanate_123 = imageconcanate.concatenate(\n",
    "            direction=\"right\",\n",
    "            match_image_size=False,\n",
    "            image1=get_value_at_index(imageresize_181, 0),\n",
    "            image2=get_value_at_index(inpaintcropimproved_183, 1),\n",
    "        )\n",
    "\n",
    "        getimagesize_102 = getimagesize.execute(\n",
    "            image=get_value_at_index(imageresize_181, 0)\n",
    "        )\n",
    "\n",
    "        cr_color_panel = NODE_CLASS_MAPPINGS[\"CR Color Panel\"]()\n",
    "        cr_color_panel_125 = cr_color_panel.make_panel(\n",
    "            panel_width=get_value_at_index(getimagesize_102, 0),\n",
    "            panel_height=get_value_at_index(getimagesize_102, 1),\n",
    "            fill_color=\"black\",\n",
    "            fill_color_hex=\"#000000\",\n",
    "        )\n",
    "\n",
    "        masktoimage_110 = masktoimage.mask_to_image(\n",
    "            mask=get_value_at_index(inpaintcropimproved_183, 2)\n",
    "        )\n",
    "\n",
    "        imageconcanate_103 = imageconcanate.concatenate(\n",
    "            direction=\"right\",\n",
    "            match_image_size=False,\n",
    "            image1=get_value_at_index(cr_color_panel_125, 0),\n",
    "            image2=get_value_at_index(masktoimage_110, 0),\n",
    "        )\n",
    "\n",
    "        imagetomask_115 = imagetomask.image_to_mask(\n",
    "            channel=\"red\", image=get_value_at_index(imageconcanate_103, 0)\n",
    "        )\n",
    "\n",
    "        growmaskwithblur = NODE_CLASS_MAPPINGS[\"GrowMaskWithBlur\"]()\n",
    "        growmaskwithblur_127 = growmaskwithblur.expand_mask(\n",
    "            expand=8,\n",
    "            incremental_expandrate=0,\n",
    "            tapered_corners=False,\n",
    "            flip_input=False,\n",
    "            blur_radius=8,\n",
    "            lerp_alpha=1,\n",
    "            decay_factor=1,\n",
    "            fill_holes=False,\n",
    "            mask=get_value_at_index(imagetomask_115, 0),\n",
    "        )\n",
    "\n",
    "        inpaintmodelconditioning = NODE_CLASS_MAPPINGS[\"InpaintModelConditioning\"]()\n",
    "        inpaintmodelconditioning_160 = inpaintmodelconditioning.encode(\n",
    "            noise_mask=False,\n",
    "            positive=get_value_at_index(conditioningconcat_121, 0),\n",
    "            negative=get_value_at_index(conditioningconcat_121, 0),\n",
    "            vae=get_value_at_index(vaeloader_107, 0),\n",
    "            pixels=get_value_at_index(imageconcanate_123, 0),\n",
    "            mask=get_value_at_index(growmaskwithblur_127, 0),\n",
    "        )\n",
    "\n",
    "        randomnoise = NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "        randomnoise_163 = randomnoise.get_noise(noise_seed=random.randint(1, 2**64))\n",
    "\n",
    "        randomnoise_225 = randomnoise.get_noise(noise_seed=random.randint(1, 2**64))\n",
    "\n",
    "        ksamplerselect_229 = ksamplerselect.get_sampler(sampler_name=\"dpmpp_2m\")\n",
    "\n",
    "        cr_upscale_image_318 = cr_upscale_image.upscale(\n",
    "            upscale_model=\"4x_NMKD-Siax_200k.pth\",\n",
    "            resampling_method=\"lanczos\",\n",
    "            supersample=\"true\",\n",
    "            image=get_value_at_index(loadimage_187, 0),\n",
    "        )\n",
    "\n",
    "        watchdetector = NODE_CLASS_MAPPINGS[\"WatchDetector\"]()\n",
    "        watchdetector_319 = watchdetector.detect_watch(\n",
    "            dp=1.2,\n",
    "            param1=100,\n",
    "            param2=80,\n",
    "            min_dist_factor=0.2,\n",
    "            min_radius_factor=0.1,\n",
    "            max_radius_factor=0.4000000000000001,\n",
    "            bg_red=220,\n",
    "            bg_green=220,\n",
    "            bg_blue=220,\n",
    "            image=get_value_at_index(cr_upscale_image_318, 0),\n",
    "        )\n",
    "\n",
    "        clipvisionencode_272 = clipvisionencode.encode(\n",
    "            crop=\"none\",\n",
    "            clip_vision=get_value_at_index(clipvisionloader_151, 0),\n",
    "            image=get_value_at_index(watchdetector_319, 1),\n",
    "        )\n",
    "\n",
    "        stylemodelapplysimple_208 = stylemodelapplysimple.apply_stylemodel(\n",
    "            image_strength=\"high\",\n",
    "            conditioning=get_value_at_index(loadconditioningnode_327, 0),\n",
    "            style_model=get_value_at_index(stylemodelloader_152, 0),\n",
    "            clip_vision_output=get_value_at_index(clipvisionencode_272, 0),\n",
    "        )\n",
    "\n",
    "        conditioningconcat_230 = conditioningconcat.concat(\n",
    "            conditioning_to=get_value_at_index(loadconditioningnode_327, 0),\n",
    "            conditioning_from=get_value_at_index(stylemodelapplysimple_208, 0),\n",
    "        )\n",
    "\n",
    "        getimagesize_134 = getimagesize.execute(\n",
    "            image=get_value_at_index(inpaintcropimproved_183, 1)\n",
    "        )\n",
    "\n",
    "        getimagesize_105 = getimagesize.execute(\n",
    "            image=get_value_at_index(imageconcanate_123, 0)\n",
    "        )\n",
    "\n",
    "        teacache = NODE_CLASS_MAPPINGS[\"TeaCache\"]()\n",
    "        teacache_156 = teacache.apply_teacache(\n",
    "            model_type=\"flux\",\n",
    "            rel_l1_thresh=0.4,\n",
    "            start_percent=0,\n",
    "            end_percent=1,\n",
    "            cache_device=\"cuda\",\n",
    "            model=get_value_at_index(unetloader_112, 0),\n",
    "        )\n",
    "\n",
    "        differentialdiffusion = NODE_CLASS_MAPPINGS[\"DifferentialDiffusion\"]()\n",
    "        differentialdiffusion_128 = differentialdiffusion.apply(\n",
    "            model=get_value_at_index(teacache_156, 0)\n",
    "        )\n",
    "\n",
    "        loraloadermodelonly = NODE_CLASS_MAPPINGS[\"LoraLoaderModelOnly\"]()\n",
    "        loraloadermodelonly_344 = loraloadermodelonly.load_lora_model_only(\n",
    "            lora_name=\"comfyui_portrait_lora64.safetensors\",\n",
    "            strength_model=0.8000000000000002,\n",
    "            model=get_value_at_index(differentialdiffusion_128, 0),\n",
    "        )\n",
    "\n",
    "        loraloadermodelonly_345 = loraloadermodelonly.load_lora_model_only(\n",
    "            lora_name=\"pytorch_lora_weights.safetensors\",\n",
    "            strength_model=0.6000000000000001,\n",
    "            model=get_value_at_index(loraloadermodelonly_344, 0),\n",
    "        )\n",
    "\n",
    "        modelsamplingflux = NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "        modelsamplingflux_129 = modelsamplingflux.patch(\n",
    "            max_shift=1.15,\n",
    "            base_shift=0.5,\n",
    "            width=get_value_at_index(getimagesize_105, 0),\n",
    "            height=get_value_at_index(getimagesize_105, 1),\n",
    "            model=get_value_at_index(loraloadermodelonly_345, 0),\n",
    "        )\n",
    "\n",
    "        fluxguidance = NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "        fluxguidance_169 = fluxguidance.append(\n",
    "            guidance=30,\n",
    "            conditioning=get_value_at_index(inpaintmodelconditioning_160, 0),\n",
    "        )\n",
    "\n",
    "        basicguider = NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "        basicguider_117 = basicguider.get_guider(\n",
    "            model=get_value_at_index(modelsamplingflux_129, 0),\n",
    "            conditioning=get_value_at_index(fluxguidance_169, 0),\n",
    "        )\n",
    "\n",
    "        basicscheduler = NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "        basicscheduler_176 = basicscheduler.get_sigmas(\n",
    "            scheduler=\"sgm_uniform\",\n",
    "            steps=30,\n",
    "            denoise=1,\n",
    "            model=get_value_at_index(modelsamplingflux_129, 0),\n",
    "        )\n",
    "\n",
    "        samplercustomadvanced = NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "        samplercustomadvanced_141 = samplercustomadvanced.sample(\n",
    "            noise=get_value_at_index(randomnoise_163, 0),\n",
    "            guider=get_value_at_index(basicguider_117, 0),\n",
    "            sampler=get_value_at_index(ksamplerselect_120, 0),\n",
    "            sigmas=get_value_at_index(basicscheduler_176, 0),\n",
    "            latent_image=get_value_at_index(inpaintmodelconditioning_160, 2),\n",
    "        )\n",
    "\n",
    "        vaedecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "        vaedecode_157 = vaedecode.decode(\n",
    "            samples=get_value_at_index(samplercustomadvanced_141, 0),\n",
    "            vae=get_value_at_index(vaeloader_107, 0),\n",
    "        )\n",
    "\n",
    "        imagecrop = NODE_CLASS_MAPPINGS[\"ImageCrop+\"]()\n",
    "        imagecrop_139 = imagecrop.execute(\n",
    "            width=get_value_at_index(getimagesize_134, 0),\n",
    "            height=get_value_at_index(getimagesize_134, 1),\n",
    "            position=\"right-center\",\n",
    "            x_offset=0,\n",
    "            y_offset=0,\n",
    "            image=get_value_at_index(vaedecode_157, 0),\n",
    "        )\n",
    "\n",
    "        layercolor_brightnesscontrastv2 = NODE_CLASS_MAPPINGS[\n",
    "            \"LayerColor: BrightnessContrastV2\"\n",
    "        ]()\n",
    "        layercolor_brightnesscontrastv2_137 = (\n",
    "            layercolor_brightnesscontrastv2.color_correct_brightness_contrast_v2(\n",
    "                brightness=1.05,\n",
    "                contrast=0.98,\n",
    "                saturation=1.05,\n",
    "                image=get_value_at_index(imagecrop_139, 0),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        masktoimage_135 = masktoimage.mask_to_image(\n",
    "            mask=get_value_at_index(growmaskwithblur_127, 0)\n",
    "        )\n",
    "\n",
    "        imagecrop_136 = imagecrop.execute(\n",
    "            width=get_value_at_index(getimagesize_134, 0),\n",
    "            height=get_value_at_index(getimagesize_134, 1),\n",
    "            position=\"right-center\",\n",
    "            x_offset=0,\n",
    "            y_offset=0,\n",
    "            image=get_value_at_index(masktoimage_135, 0),\n",
    "        )\n",
    "\n",
    "        imagetomask_138 = imagetomask.image_to_mask(\n",
    "            channel=\"red\", image=get_value_at_index(imagecrop_136, 0)\n",
    "        )\n",
    "\n",
    "        imagecompositemasked = NODE_CLASS_MAPPINGS[\"ImageCompositeMasked\"]()\n",
    "        imagecompositemasked_191 = imagecompositemasked.composite(\n",
    "            x=0,\n",
    "            y=0,\n",
    "            resize_source=False,\n",
    "            destination=get_value_at_index(inpaintcropimproved_183, 1),\n",
    "            source=get_value_at_index(layercolor_brightnesscontrastv2_137, 0),\n",
    "            mask=get_value_at_index(imagetomask_138, 0),\n",
    "        )\n",
    "\n",
    "        inpaintstitchimproved = NODE_CLASS_MAPPINGS[\"InpaintStitchImproved\"]()\n",
    "        inpaintstitchimproved_178 = inpaintstitchimproved.inpaint_stitch(\n",
    "            stitcher=get_value_at_index(inpaintcropimproved_183, 0),\n",
    "            inpainted_image=get_value_at_index(imagecompositemasked_191, 0),\n",
    "        )\n",
    "\n",
    "        cr_upscale_image_186 = cr_upscale_image.upscale(\n",
    "            upscale_model=\"4x_NMKD-Siax_200k.pth\",\n",
    "            resampling_method=\"lanczos\",\n",
    "            supersample=\"true\",\n",
    "            image=get_value_at_index(inpaintstitchimproved_178, 0),\n",
    "        )\n",
    "\n",
    "        watchdetector_312 = watchdetector.detect_watch(\n",
    "            dp=1.2,\n",
    "            param1=100,\n",
    "            param2=80,\n",
    "            min_dist_factor=0.2,\n",
    "            min_radius_factor=0.010000000000000002,\n",
    "            max_radius_factor=0.4,\n",
    "            bg_red=220,\n",
    "            bg_green=220,\n",
    "            bg_blue=220,\n",
    "            image=get_value_at_index(cr_upscale_image_186, 0),\n",
    "        )\n",
    "\n",
    "        inpaintcropimproved_258 = inpaintcropimproved.inpaint_crop(\n",
    "            downscale_algorithm=\"bilinear\",\n",
    "            upscale_algorithm=\"bicubic\",\n",
    "            preresize=False,\n",
    "            preresize_mode=\"ensure minimum resolution\",\n",
    "            preresize_min_width=1024,\n",
    "            preresize_min_height=1024,\n",
    "            preresize_max_width=16384,\n",
    "            preresize_max_height=16384,\n",
    "            mask_fill_holes=True,\n",
    "            mask_expand_pixels=0,\n",
    "            mask_invert=False,\n",
    "            mask_blend_pixels=32,\n",
    "            mask_hipass_filter=0.1,\n",
    "            extend_for_outpainting=False,\n",
    "            extend_up_factor=1,\n",
    "            extend_down_factor=1,\n",
    "            extend_left_factor=1,\n",
    "            extend_right_factor=1,\n",
    "            context_from_mask_extend_factor=1.2000000000000002,\n",
    "            output_resize_to_target_size=True,\n",
    "            output_target_width=1080,\n",
    "            output_target_height=1080,\n",
    "            output_padding=\"128\",\n",
    "            image=get_value_at_index(cr_upscale_image_186, 0),\n",
    "            mask=get_value_at_index(watchdetector_312, 0),\n",
    "        )\n",
    "\n",
    "        getimagesize_212 = getimagesize.execute(\n",
    "            image=get_value_at_index(inpaintcropimproved_258, 1)\n",
    "        )\n",
    "\n",
    "        imageresize_213 = imageresize.execute(\n",
    "            width=16384,\n",
    "            height=get_value_at_index(getimagesize_212, 1),\n",
    "            interpolation=\"lanczos\",\n",
    "            method=\"keep proportion\",\n",
    "            condition=\"always\",\n",
    "            multiple_of=0,\n",
    "            image=get_value_at_index(watchdetector_319, 1),\n",
    "        )\n",
    "\n",
    "        imageconcanate_232 = imageconcanate.concatenate(\n",
    "            direction=\"right\",\n",
    "            match_image_size=False,\n",
    "            image1=get_value_at_index(imageresize_213, 0),\n",
    "            image2=get_value_at_index(inpaintcropimproved_258, 1),\n",
    "        )\n",
    "\n",
    "        getimagesize_204 = getimagesize.execute(\n",
    "            image=get_value_at_index(imageresize_213, 0)\n",
    "        )\n",
    "\n",
    "        cr_color_panel_234 = cr_color_panel.make_panel(\n",
    "            panel_width=get_value_at_index(getimagesize_204, 0),\n",
    "            panel_height=get_value_at_index(getimagesize_204, 1),\n",
    "            fill_color=\"black\",\n",
    "            fill_color_hex=\"#000000\",\n",
    "        )\n",
    "\n",
    "        masktoimage_211 = masktoimage.mask_to_image(\n",
    "            mask=get_value_at_index(inpaintcropimproved_258, 2)\n",
    "        )\n",
    "\n",
    "        imageconcanate_205 = imageconcanate.concatenate(\n",
    "            direction=\"right\",\n",
    "            match_image_size=False,\n",
    "            image1=get_value_at_index(cr_color_panel_234, 0),\n",
    "            image2=get_value_at_index(masktoimage_211, 0),\n",
    "        )\n",
    "\n",
    "        imagetomask_221 = imagetomask.image_to_mask(\n",
    "            channel=\"red\", image=get_value_at_index(imageconcanate_205, 0)\n",
    "        )\n",
    "\n",
    "        growmaskwithblur_236 = growmaskwithblur.expand_mask(\n",
    "            expand=8,\n",
    "            incremental_expandrate=0,\n",
    "            tapered_corners=False,\n",
    "            flip_input=False,\n",
    "            blur_radius=8,\n",
    "            lerp_alpha=1,\n",
    "            decay_factor=1,\n",
    "            fill_holes=False,\n",
    "            mask=get_value_at_index(imagetomask_221, 0),\n",
    "        )\n",
    "\n",
    "        inpaintmodelconditioning_239 = inpaintmodelconditioning.encode(\n",
    "            noise_mask=False,\n",
    "            positive=get_value_at_index(conditioningconcat_230, 0),\n",
    "            negative=get_value_at_index(conditioningconcat_230, 0),\n",
    "            vae=get_value_at_index(vaeloader_107, 0),\n",
    "            pixels=get_value_at_index(imageconcanate_232, 0),\n",
    "            mask=get_value_at_index(growmaskwithblur_236, 0),\n",
    "        )\n",
    "\n",
    "        upscalemodelloader = NODE_CLASS_MAPPINGS[\"UpscaleModelLoader\"]()\n",
    "        upscalemodelloader_322 = upscalemodelloader.load_model(\n",
    "            model_name=\"4x_NMKD-Siax_200k.pth\"\n",
    "        )\n",
    "\n",
    "        upscalemodelloader_333 = upscalemodelloader.load_model(\n",
    "            model_name=\"4x_NMKD-Siax_200k.pth\"\n",
    "        )\n",
    "\n",
    "        maskpreview = NODE_CLASS_MAPPINGS[\"MaskPreview+\"]()\n",
    "        imageupscalewithmodel = NODE_CLASS_MAPPINGS[\"ImageUpscaleWithModel\"]()\n",
    "\n",
    "        for q in range(1):\n",
    "            getimagesize_207 = getimagesize.execute(\n",
    "                image=get_value_at_index(imageconcanate_232, 0)\n",
    "            )\n",
    "\n",
    "            fluxguidance_223 = fluxguidance.append(\n",
    "                guidance=50,\n",
    "                conditioning=get_value_at_index(inpaintmodelconditioning_239, 0),\n",
    "            )\n",
    "\n",
    "            modelsamplingflux_238 = modelsamplingflux.patch(\n",
    "                max_shift=1.15,\n",
    "                base_shift=0.5,\n",
    "                width=get_value_at_index(getimagesize_207, 0),\n",
    "                height=get_value_at_index(getimagesize_207, 1),\n",
    "                model=get_value_at_index(loraloadermodelonly_345, 0),\n",
    "            )\n",
    "\n",
    "            basicguider_224 = basicguider.get_guider(\n",
    "                model=get_value_at_index(modelsamplingflux_238, 0),\n",
    "                conditioning=get_value_at_index(fluxguidance_223, 0),\n",
    "            )\n",
    "\n",
    "            getimagesize_245 = getimagesize.execute(\n",
    "                image=get_value_at_index(inpaintcropimproved_258, 1)\n",
    "            )\n",
    "\n",
    "            masktoimage_246 = masktoimage.mask_to_image(\n",
    "                mask=get_value_at_index(growmaskwithblur_236, 0)\n",
    "            )\n",
    "\n",
    "            imagecrop_247 = imagecrop.execute(\n",
    "                width=get_value_at_index(getimagesize_245, 0),\n",
    "                height=get_value_at_index(getimagesize_245, 1),\n",
    "                position=\"right-center\",\n",
    "                x_offset=0,\n",
    "                y_offset=0,\n",
    "                image=get_value_at_index(masktoimage_246, 0),\n",
    "            )\n",
    "\n",
    "            imagetomask_249 = imagetomask.image_to_mask(\n",
    "                channel=\"red\", image=get_value_at_index(imagecrop_247, 0)\n",
    "            )\n",
    "\n",
    "            basicscheduler_270 = basicscheduler.get_sigmas(\n",
    "                scheduler=\"sgm_uniform\",\n",
    "                steps=30,\n",
    "                denoise=1,\n",
    "                model=get_value_at_index(modelsamplingflux_238, 0),\n",
    "            )\n",
    "\n",
    "            samplercustomadvanced_253 = samplercustomadvanced.sample(\n",
    "                noise=get_value_at_index(randomnoise_225, 0),\n",
    "                guider=get_value_at_index(basicguider_224, 0),\n",
    "                sampler=get_value_at_index(ksamplerselect_229, 0),\n",
    "                sigmas=get_value_at_index(basicscheduler_270, 0),\n",
    "                latent_image=get_value_at_index(inpaintmodelconditioning_239, 2),\n",
    "            )\n",
    "\n",
    "            vaedecode_278 = vaedecode.decode(\n",
    "                samples=get_value_at_index(samplercustomadvanced_253, 0),\n",
    "                vae=get_value_at_index(vaeloader_107, 0),\n",
    "            )\n",
    "\n",
    "            imagecrop_251 = imagecrop.execute(\n",
    "                width=get_value_at_index(getimagesize_245, 0),\n",
    "                height=get_value_at_index(getimagesize_245, 1),\n",
    "                position=\"right-center\",\n",
    "                x_offset=0,\n",
    "                y_offset=0,\n",
    "                image=get_value_at_index(vaedecode_278, 0),\n",
    "            )\n",
    "\n",
    "            imagecompositemasked_273 = imagecompositemasked.composite(\n",
    "                x=0,\n",
    "                y=0,\n",
    "                resize_source=False,\n",
    "                destination=get_value_at_index(inpaintcropimproved_258, 1),\n",
    "                source=get_value_at_index(imagecrop_251, 0),\n",
    "                mask=get_value_at_index(imagetomask_249, 0),\n",
    "            )\n",
    "\n",
    "            inpaintstitchimproved_262 = inpaintstitchimproved.inpaint_stitch(\n",
    "                stitcher=get_value_at_index(inpaintcropimproved_258, 0),\n",
    "                inpainted_image=get_value_at_index(imagecompositemasked_273, 0),\n",
    "            )\n",
    "\n",
    "            maskpreview_287 = maskpreview.execute(\n",
    "                mask=get_value_at_index(watchdetector_312, 0)\n",
    "            )\n",
    "\n",
    "            imageupscalewithmodel_324 = imageupscalewithmodel.upscale(\n",
    "                upscale_model=get_value_at_index(upscalemodelloader_322, 0),\n",
    "                image=get_value_at_index(inpaintstitchimproved_262, 0),\n",
    "            )\n",
    "\n",
    "            maskpreview_317 = maskpreview.execute(\n",
    "                mask=get_value_at_index(watchdetector_319, 0)\n",
    "            )\n",
    "\n",
    "            imageupscalewithmodel_334 = imageupscalewithmodel.upscale(\n",
    "                upscale_model=get_value_at_index(upscalemodelloader_333, 0),\n",
    "                image=get_value_at_index(inpaintstitchimproved_178, 0),\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
